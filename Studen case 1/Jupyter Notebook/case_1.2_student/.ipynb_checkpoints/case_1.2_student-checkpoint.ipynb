{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are trading volume and volatility related for energy stocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "By the end of this case, we will have introduced the `pandas` library within Python. You will also have gained experience with the `numpy` library, know how to read data files, and conduct descriptive statistics.\n",
    "\n",
    "You should also begin to develop a proper mindset for investigating the library on your own, via documentation or other resources such as StackOverflow. Self-research of existing documentation is a crucial part of developing as a data professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Business Context.** You are an analyst at a large bank focused on natural resource stock investments. Natural resources are vital for a variety of industries in our economy. Recently, your division has taken interest in the following stocks:\n",
    "\n",
    "1. Dominion Energy Inc.\n",
    "2. Exelon Corp.\n",
    "3. NextEra Energy Inc.\n",
    "4. Southern Co.\n",
    "5. Duke Energy Corp.\n",
    "\n",
    "These stocks are all part of the energy sector, an important but volatile sector of the stock market. While high volatility increases the chance of great gains, it also makes it more likely to have large losses, so risk must be carefully managed with high-volatility stocks.\n",
    "\n",
    "Because your firm is quite large, there must be enough trading volume (average amount of shares transacted per day) so that it can easily transact in these stocks. Otherwise, this effect compounded with the stocks' naturally high volatility could make these too risky for the bank to invest in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem.** Given that both low trading volume and high volatility present risks to your investments, your team lead asks you to investigate the following: **\"How is the volatility of energy stocks related to their average daily trading volume?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Context.** The data you've been given is in the [Comma Separated Value (CSV) format](https://frictionlessdata.io/docs//), and comprises price and trading volume data for the above stocks. This case begins with a brief overview of this data, after which you will: (1) learn how to use the Python library [pandas](https://towardsdatascience.com/a-quick-introduction-to-the-pandas-python-library-f1b678f34673) to load the data; (2) use ```pandas``` transform this data into a form amenable for analysis; and finally (3) use ```pandas``` to analyze the above question and come to a conclusion. As you may have guessed, ```pandas``` is an enormously useful library for data analysis and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages to aid in data analysis\n",
    "\n",
    "[External libraries (a.k.a. packages)](https://www.learnpython.org/en/Modules_and_Packages) are code bases that contain a variety of pre-written functions and tools. This allows you to perform a variety of complex tasks in Python without having to \"reinvent the wheel\" build everything from the ground up. We will use two core packages: ```pandas``` and ```numpy```.\n",
    "\n",
    "```pandas``` is an external library that provides functionality for data analysis. Pandas specifically offers a variety of data structures and data manipulation methods that allow you to perform complex tasks with simple, one-line commands.\n",
    "\n",
    "```numpy``` is a package that we will use later in the case that offers numerous mathematical operations. Together, [pandas](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v1.0.0.html) and [numpy](https://numpy.org/) allow you to create a data science workflow within Python. `numpy` is in many ways foundational to `pandas`, providing vectorized operations, while `pandas` provides higher level abstractions built on top of `numpy`.</font>\n",
    "\n",
    "Let's import both packages using the ```import``` keyword. We will rename ```pandas``` to ```pd``` and ```numpy``` to ```np``` using the ```as``` keyword. This allows us to use the short name abbreviation when we want to reference any function that is inside either package. The abbreviations we chose are standard across the data science industry and should be followed unless there is a very good reason not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Import the NumPy package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these packages are loaded into Python, we can use their contents. Let's first take a look at ```pandas``` as it has a variety of features we will use to load and analyze our stock data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of ```pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`pandas` is a Python library that facilitates a wide range of data analysis and manipulation. Before, you saw basic data structures in Python such as lists and dictionaries. While you can build a basic data table (similar to an Excel spreadsheet) using nested lists in Python, they get quite difficult to work with. By contrast, in `pandas` the table data structure, known as a `DataFrame`, is a first-class citizen and you can easily manipulate your data thinking of it in rows and columns.\n",
    "\n",
    "If you've ever used or heard of R or SQL before, `pandas` brings some functionality from each of these to Python, allowing you to structure and filter data more efficiently than pure Python. This efficiency is seen in two distinct ways:\n",
    "\n",
    "* Scripts written using `pandas` will often run faster than scripts written in pure Python\n",
    "* Scripts written using `pandas` will often contain far fewer lines of code than the equivalent script written in pure Python.\n",
    "\n",
    "At the core of the ```pandas``` library are two fundamental data structures/objects:\n",
    "1. [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)\n",
    "2. [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n",
    "\n",
    "A ```Series``` object stores single-column data along with an **index**. An index is just a way of \"numbering\" the ```Series``` object. For example, in this case study, the indices will be dates, while the single-column data may be stock prices or daily trading volume.\n",
    "\n",
    "A ```DataFrame``` object is a two-dimensional tabular data structure with labeled axes. It is conceptually helpful to think of a DataFrame object as a collection of Series objects. Namely, think of each column in a DataFrame as a single Series object, where each of these Series objects shares a common index -  the index of the DataFrame object.\n",
    "\n",
    "Below is the syntax for creating a Series object, followed by the syntax for creating a DataFrame object. Note that DataFrame objects can also have a single-column â€“ think of this as a DataFrame consisting of a single Series object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Series object\n",
    "simple_series = pd.Series(\n",
    "    index=[0, 1, 2, 3], name=\"Volume\", data=[1000, 2600, 1524, 98000]\n",
    ")\n",
    "simple_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing ```pd.Series``` to ```pd.DataFrame```, and adding a columns input list, a DataFrame object can be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame object\n",
    "simple_df = pd.DataFrame(\n",
    "    index=[0, 1, 2, 3], columns=[\"Volume\"], data=[1000, 2600, 1524, 98000]\n",
    ")\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame objects are more general than Series objects, and one DataFrame can hold many Series objects, each as a different column. Let's create a two-column DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another DataFrame object\n",
    "another_df = pd.DataFrame(\n",
    "    index=[0, 1, 2, 3],\n",
    "    columns=[\"Date\", \"Volume\"],\n",
    "    data=[[20190101, 1000], [20190102, 2600], [20190103, 1524], [20190104, 98000]],\n",
    ")\n",
    "another_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a list of lists was used to specify the data in the ```another_df``` DataFrame. Each element of the list corresponds to a row in the DataFrame, so the list has 4 elements because there are 4 indices. Each element of the list of lists has 2 elements because the DataFrame has two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using <code>pandas</code> to analyze stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recall that we have CSV files that include data for each of the following stocks:\n",
    "\n",
    "1. Dominion Energy Inc. (Stock Symbol: D)\n",
    "2. Exelon Corp. (Stock Symbol: EXC)\n",
    "3. NextEra Energy Inc. (Stock Symbol: NEE)\n",
    "4. Southern Co. (Stock Symbol: SO)\n",
    "5. Duke Energy Corp. (Stock Symbol: DUK)\n",
    "\n",
    "The available data for each stock includes:\n",
    "\n",
    "1. **Date:** The day of the year\n",
    "2. **Open:** The stock opening price of the day\n",
    "3. **High:** The highest observed stock price of the day\n",
    "4. **Low:** The lowest observed stock price of the day\n",
    "5. **Close:** The stock closing price of the day\n",
    "6. **Adj Close:** The adjusted stock closing price for the day (adjusted for splits and dividends)\n",
    "7. **Volume:** The volume of the stock traded over the day\n",
    "\n",
    "To get a better sense of the available data, let's first take a look at just the data for Dominion Energy, listed on the New York Stock Exchange under the symbol D. You are given a CSV file that contains the company's stock data, ```D.```. `pandas` allows easy loading of CSV files through the use of the method [pd.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a file as a DataFrame and assign to df\n",
    "df = pd.read_csv(\"data/D.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the file ```D.``` are now stored in the DataFrame object ```df```.\n",
    "\n",
    "There are several common methods and attributes available to take a peek at the data and get a sense of it:\n",
    "\n",
    "1. ```DataFrame.head()```  -> returns the column names and first 5 rows by default\n",
    "2. ```DataFrame.tail()```  -> returns the column names and last 5 rows by default\n",
    "3. ```DataFrame.shape```   -> returns (num_rows, num_columns)\n",
    "4. ```DataFrame.columns``` -> returns index of columns\n",
    "5. ```DataFrame.index```   -> returns index of rows\n",
    "\n",
    "In your spare time please check the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/) and explore the parameters of these methods as well as other methods. Familiarity with this library will dramatically improve your productivity as a data scientist.\n",
    "\n",
    "Using ```df.head()``` and ```df.tail()``` we can take a look at the data contents. Unless specified otherwise, Series and DataFrame objects have indices starting at 0 and increase monotonically upward along the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the head of the DataFrame (i.e. the top rows of the DataFrame)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the tail of the DataFrame (i.e. the top rows of the DataFrame)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see there are 1259 data entries (each with 7 data points) for Dominion Energy. The shape of a DataFrame is accessed using the ```shape``` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the shape of the two-dimensional structure, that is (num_rows, num_columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that ```DataFrame.columns``` and ```DataFrame.index``` return an index object instead of a list. To cast an index to a list for further list manipulation, we use the ```list()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the column names of the DataFrame\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of the column names of the DataFrame\n",
    "list(df.index)[0:20]  # only showing first 20 index values so reduce screen output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating additional variables relevant to stock volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, the data provided to you will not be sufficient to achieve your goal. You may have to add additional variables or data features to assist you. Recall that our original question concerned the relationship between stock trading volume and volatility. Therefore, our DataFrame must have features related to both of these quantities.\n",
    "\n",
    "It can be helpful to think about adding columns to DataFrames as adding adjacent columns one-by-one in Excel. Here is an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column named \"Symbol\"\n",
    "df[\"Symbol\"] = \"D\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access a column by using [] brackets and the column name\n",
    "df['Volume'].head() # added .head() to suppress output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column named \"Volume_Millions\", which is calculated from the Volume column currently in df\n",
    "# divide every row in df['Volume'] by 1 million, store in new column\n",
    "df[\"Volume_Millions\"] = df[\"Volume\"] / 1000000.0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the updated DataFrame shape. Two new columns have been added.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, we need to have a feature in our DataFrame that is related to volatility. Because this currently does not exist, we must create it from the already available features. Recall that volatility is the standard deviation of daily returns over a period of time, so let's create a feature for daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VolStat\"] = (df[\"High\"] - df[\"Low\"]) / df[\"Open\"]\n",
    "df[\"Return\"] = (df[\"Close\"] / df[\"Open\"]) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the power of ```pandas```. We can simply perform mathematical operations on columns of DataFrames just as if the DataFrames were single variables themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have features relevant to the original question, and can proceed to the analysis step. A common first step in data analysis is to learn about the distribution of the available data. We will do this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about the data distribution through summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's aggregate summary statistics for the five energy sector companies under study. Fortunately, the DataFrame and Series objects offer a myriad of data summary statistics methods:\n",
    "\n",
    "1. ```min()```\n",
    "2. ```median()```\n",
    "3. ```mean()```\n",
    "4. ```max()```\n",
    "5. ```quantile()```\n",
    "\n",
    "Below, each method is used on the ```Volume_Millions``` column. Notice how simple the functions are to apply to the DataFrame. Simply type the name of the DataFrame, followed by a ```.``` and then the method name you'd like to calculate. We've chosen to select a single column ```Volume_Millions``` from the DataFrame ```df```, but you could have just as easily called these methods on the full DataFrame rather than a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum of the Volume_Millions column\n",
    "df[\"Volume_Millions\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of the Volume_Millions column\n",
    "df[\"Volume_Millions\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of the Volume_Millions column\n",
    "df[\"Volume_Millions\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the maximum of the Volume_Millions column\n",
    "df[\"Volume_Millions\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like to explore the data distribution at a more granular level to see how the distribution looks beyond the simple summary statistics presented above. For this, we can use the ```quantile()``` method. The ```quantile()``` method will return the value which represents the given percentile of all the data under study (in this case, of the ```Volume_Millions``` data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 25th percentile\n",
    "df['Volume_Millions'].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 75th percentile\n",
    "df['Volume_Millions'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a more efficient method to quickly compute all of these summary statistics? Yes. One incredibly useful method that combines these summary statistics and also adds a couple others is the [describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Volume_Millions'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this distribution analysis of the daily trading volume, we can see that more than 14 million shares would be a very large trading day, whereas below 2 million shares would be a relatively small trading day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to describe, there is a [value_counts() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) for checking the frequency of elements in categorical data. Please be aware that `value_counts()` is a method of the Series class and NOT the DataFrame class. This means you have to isolate a specific column of a DataFrame before calling `value_counts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {\n",
    "    \"numbers\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"color\": [\"red\", \"red\", \"red\", \"blue\", \"blue\", \"green\", \"blue\", \"green\"],\n",
    "}\n",
    "category_df = pd.DataFrame(data=dict_data)\n",
    "\n",
    "category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why doesn't this work? (uncomment the expression that follows)\n",
    "#category_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only Series objects can call this method (uncomment the following expression)\n",
    "# category_df['color'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Determine the 25th, 50th, and 75th percentile for the ```Open```, ```High```, ```Low```, and ```Close``` columns of ```df```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data from multiple companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So far, we've only been looking at data from one of our five companies. Let's go ahead and combine all five CSV files to analyze the five companies together. This will also reduce the amount of programming work required since the code will be shared across the five companies.\n",
    "\n",
    "One way to accomplish this aggregation task is to use the [pd.concat()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) method from ```pandas```. An input into this method may be a list of DataFrames that you'd like to concatenate. We will use a `for` loop to loop over each stock symbol, load the corresponding CSV file, and then append the result to a list which is later aggregated using ```pd.concat()```. Let's take a look at how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load five  files into one dataframe\n",
    "print(\"Defining stock symbols\")\n",
    "symbol_data_to_load = [\"D\", \"EXC\", \"NEE\", \"SO\", \"DUK\"]\n",
    "list_of_df = []\n",
    "\n",
    "# Loop over all symbols\n",
    "print(\" --- Start loop over symbols --- \")\n",
    "for symbol in symbol_data_to_load:\n",
    "    print(\"Processing Symbol: \" + symbol)\n",
    "    temp_df = pd.read_csv(\"data/\" + symbol + \".csv\")\n",
    "    temp_df[\"Volume_Millions\"] = temp_df[\"Volume\"] / 1000000.0\n",
    "\n",
    "    # Add new column with symbol name to distinguish in final dataframe\n",
    "    temp_df[\"Symbol\"] = symbol\n",
    "    list_of_df.append(temp_df)\n",
    "\n",
    "# used a line break at the end of this string for aesthetics\n",
    "print(\" --- Complete loop over symbols --- \\n\")\n",
    "\n",
    "# Combine into a single DataFrame by using concat\n",
    "print(\"Aggregating Data\")\n",
    "agg_df = pd.concat(list_of_df, axis=0)\n",
    "\n",
    "# Add salient statistics for this return and volatility analysis\n",
    "print(\"Calculating Salient Features\")\n",
    "agg_df[\"VolStat\"] = (agg_df[\"High\"] - agg_df[\"Low\"]) / agg_df[\"Open\"]\n",
    "agg_df[\"Return\"] = (agg_df[\"Close\"] / agg_df[\"Open\"]) - 1.0\n",
    "\n",
    "print(\"agg_df DataFrame shape (rows, columns): \")\n",
    "print(agg_df.shape)\n",
    "\n",
    "print(\"Head of agg_df DataFrame: \")\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `for` loop, we've aggregated and added the relevant features we identified in the previous section. We then printed the head of the aggregated DataFrame to have a peek at the format of the data, and we've also printed the shape of the DataFrame. This is to sanity check that our final DataFrame is roughly what we expect. Notice the aggregated DataFrame has the same number of columns as the original single stock (D) data, however the number of rows have increased five-fold. This makes sense, because each additional symbol contains 1259 data entries, so five symbols leads to a total of ```1259*5 = 6295``` rows. So, this passes our sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to reverse this process and extract the data relevant to a single stock symbol from the aggregated DataFrame ```agg_df```, we can do so using the ```==``` operator, which returns True when two objects contain the same value, and False otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_DUK_df = agg_df[agg_df[\"Symbol\"] == \"DUK\"]\n",
    "symbol_DUK_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the code block above, we've filtered out the rows that correspond to each symbol. Namely,\n",
    "\n",
    "```python\n",
    "agg_df['Symbol'] == 'DUK'\n",
    "```\n",
    "returns a boolean series of the same number of rows of ```agg_df```, where each value is True or False depending on whether a specific row's ```Symbol``` value is equal to ```'DUK'```.\n",
    "\n",
    "This row extraction technique will be useful to us later in this case when we perform analyses on each individual stock symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "If we added the number of rows together from the five DataFrames, ```D_df```,```NEE_df```,```EXC_df```,```SO_df```, and ```DUK_df```, we'd arrive at the same number of rows as ```agg_df```: 6295 rows. If we instead used the ```!=``` operator in the five lines where we filter out each symbol, how many rows would we have if we sum all the rows in the five new DataFrames?\n",
    "\n",
    "(a) 31475\n",
    "\n",
    "(b) 12590\n",
    "\n",
    "(c) 25180\n",
    "\n",
    "(d) 6295\n",
    "\n",
    "**Answer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: \n",
    "\n",
    "Write code to write a `for` loop to loop through each of the five symbols, extract only the rows corresponding to each symbol, and calculate and print the average ```VolStat``` value for each of the five symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing each stock's volatility levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```pandas``` offers the ability to group related rows of DataFrames according to the values of other rows. This useful feature is accomplished using the [groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) method.  Let's take a look and see how this can be used to group rows so that each group corresponds to a single stock symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the groupby() method, notice a DataFrameGroupBy object is returned\n",
    "agg_df.groupby('Symbol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, the ```DataFrameGroupBy``` object can be most readily thought of as containing a DataFrame object for every group (in this case, a DataFrame object for each symbol). Specifically, each item of the object is a tuple, containing the group identifier (in this case the Symbol), and the corresponding rows of the DataFrame that have that Symbol).\n",
    "\n",
    "Fortunately, ```pandas``` allows you to iterate over the `groupby()` object to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_obj = agg_df.groupby(\"Symbol\")  # Group data in agg_df by Symbol\n",
    "\n",
    "# Loop through groups\n",
    "for item in grp_obj:\n",
    "    print(\" ------ Loop Begins ------ \")\n",
    "    print(type(item))  # Showing type of the item in grp_obj\n",
    "    print(item[0])  # Symbol\n",
    "    print(item[1].head())  # DataFrame with data for the Symbol\n",
    "    print(\" ------ Loop Ends ------ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the ```pd.groupby()``` method with the ```describe()``` method and apply it to each symbol to analyze the distribution of volatility related features for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_obj = agg_df.groupby(\"Symbol\")  # Group data in agg_df by Symbol\n",
    "\n",
    "# Loop through groups\n",
    "for item in grp_obj:\n",
    "    print(\"------Symbol: \", item[0])\n",
    "    grp_df = item[1]\n",
    "    relevant_df = grp_df[[\"VolStat\"]]\n",
    "    print(relevant_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediate observation of note is that the volatility level on any given day can vary widely. This is evident from the wide spread between the minimum and maximum ```VolStat``` levels seen using the ```describe()``` method. For example, stock symbol D has a minimum ```VolStat``` value of 0.003640, while its maximum ```VolStat``` value is 0.062350. That's more than a 10x increase in the value of ```VolStat```!\n",
    "\n",
    "While this is great to see, there is a more powerful way to display this data in `pandas`. We can call the ```describe()``` method directly on the ```DataFrameGroupBy``` object. This one line allows you to avoid having to write a `for` loop every time you'd like to summarize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VolStat\n",
    "agg_df[[\"Symbol\", \"VolStat\"]].groupby(\"Symbol\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is identical to the data previously outputted using the `for` loop approach. The difference is that utilizing the features of the ```DataFrameGroupBy``` object allows for easy coding, fast results, and a clean output. This illustrates the power of using the ```pd.groupby()``` method: generating statistics for groups of interest in your data is straightforward and efficient to code.\n",
    "\n",
    "You'll notice this pattern a lot as you gain more familiarity with Python and data analysis. There are many ways to solve a problem, but often one way is substantially more efficient, both in terms of run time and in terms of lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "What are some insights you can draw from the ```VolStat``` summary statistics in terms of volatility levels?\n",
    "\n",
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Using ```agg_df``` and a `for` loop, write a script to determine the mean value of ```VolStat``` for each symbol by year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling data points as high or low volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've determined that the volatility levels of each stock can vary widely, the next logical step is to group periods of high and low volatility so that we can then look at how volume differs between those time periods.\n",
    "\n",
    "However, we don't currently have a column that identifies when volatility is high and when it is low. Therefore, we must create a new column called ```VolLevel``` using some volatility threshold. For example, we'd like to have a new column value determined by:\n",
    "\n",
    "```\n",
    "if VolStat > threshold:\n",
    "    VolLevel = 'HIGH'\n",
    "else:\n",
    "    VolLevel = 'LOW'\n",
    "```\n",
    "\n",
    "Here we will define low volatility levels by any ```VolStat``` below the 50th percentile (i.e. below the median level of volatility for that symbol). Each percentile value must be calculated by symbol to ensure that each symbol is individually analyzed.\n",
    "\n",
    "Let's take a look how we can accomplish this task using ```groupby()``` functionality and the ```quantile()``` method, which returns the percentile for a given series of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine lower thresholds for volatility for each symbol\n",
    "volstat_thresholds = agg_df.groupby(\"Symbol\")[\"VolStat\"].quantile(0.5)  # 50th percentile (median)\n",
    "print(volstat_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'd like to label periods of high and low volatility by symbol, we will make use of the [np.where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html) method in the ```numpy``` library. This method takes an input and checks a logical condition: if the condition is true, it will return its second argument, whereas if the condition is false, it will return its third argument. This is very similar to how Microsoft Excel's ```IFERROR()``` method works (helpful to think of it this way for those familiar with Excel). Let's loop through each symbol and label each day as either high and low volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through symbols\n",
    "print(\"Defining stock symbols\")\n",
    "list_of_symbols = [\"D\", \"EXC\", \"NEE\", \"SO\", \"DUK\"]\n",
    "list_of_df = []\n",
    "\n",
    "# Loop over all symbols\n",
    "print(\" --- Loop over symbols --- \")\n",
    "for i in symbol_data_to_load:\n",
    "    print(\"Labelling Volatility regime for Symbol: \" + i)\n",
    "    temp_df = agg_df[agg_df[\"Symbol\"] == i].copy()  # make a copy of the dataframe to ensure not affecting agg_df\n",
    "    volstat_t = volstat_thresholds.loc[i]\n",
    "    temp_df[\"VolLevel\"] = np.where(temp_df[\"VolStat\"] < volstat_t, \"LOW\", \"HIGH\")  # Volatility regime label\n",
    "    list_of_df.append(temp_df)\n",
    "\n",
    "print(\" --- Completed loop over symbols --- \")\n",
    "\n",
    "print(\"Aggregating data\")\n",
    "labeled_df = pd.concat(list_of_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now added a ```VolLevel``` column that identifies whether each symbol is in a period of high or low volatility on any given day. Since we know that the bank will require higher trading volume in order to transact in periods of high volatility, let's now take a look at the average daily traded volume for high volatility vs. low volatility days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is daily trading volume affected by the level of volatility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the relationship between volatility level and daily trading volume, let's group by ```VolLevel``` and take a look at the average ```Volume``` for the high and low volatility groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_df.groupby([\"Symbol\", \"VolLevel\"])[[\"Volume_Millions\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "What is an immediate trend you notice regarding the volatility regimes?\n",
    "\n",
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: \n",
    "\n",
    "Write code to group time periods into low, medium, and high volatility regimes, where:\n",
    "\n",
    "```\n",
    "if VolStat > (75th percentile VolStat for given symbol):\n",
    "    VolLevel = 'HIGH'\n",
    "elif  VolStat > (25th percentile VolStat for given symbol):\n",
    "    VolLevel = 'MEDIUM'\n",
    "else:\n",
    "    VolLevel = 'LOW'\n",
    "```\n",
    "\n",
    "Output a ```final_df``` DataFrame output grouped by `Symbol`, showing the mean `Volume` for each `VolLevel` category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we use ```loc``` to index the DataFrame object. This is just one of many different ways to slice your DataFrame object. We recommend looking into [loc vs iloc](https://www.pythonprogramming.in/what-is-difference-between-iloc-and-loc-in-pandas.html) as both will be useful for all data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing volatility across time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now satisfactorily answered our original question. However, you don't need to just analyze data in tabular format. Python contains functionality to allow you to analyze your data visually as well.\n",
    "\n",
    "We will use ```pandas``` functionality built on the standard Python plotting library [matplotlib](https://matplotlib.org/). Let's import the library and instruct Jupyter to display the plots inline (i.e. display the plots to the notebook screen so we can see them as we run the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fundamental plotting library in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instruct jupyter to plot in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot, we need to convert the ```Date``` column in ```agg_df``` to a ```datetime```-like object, Python's internal data representation of dates. ```pandas``` offers the [to_datetime()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) method to convert a string that represents a given date format into a ```datetime```-like object. We instruct ```pandas``` to use ```format='%Y-%m-%d'```, since our dates are in this format, where %Y indicates the numerical year, %m indicates the numerical month and %d indicates the numerical day. If our dates were in another format, we'd modify this input value appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert a string to a datetime\n",
    "agg_df[\"DateTime\"] = pd.to_datetime(agg_df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Set index as DateTime for plotting purposes\n",
    "agg_df = agg_df.set_index([\"DateTime\"])\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to look directly at volatility across time. Let's group by symbols and plot the ```VolStat``` value across time. Each symbol's time series will be labelled a different color by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at volatility regimes\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "agg_df.groupby(\"Symbol\")[\"VolStat\"].plot(\n",
    "    ax=ax, legend=True, title=\"Energy Sector Trends - VolStat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that periods of high volatility tend to \"clump\" together; that is, periods of high volatility are not uniformly and randomly distributed across time, but tend to occur in highly concentrated bursts. This is an interesting insight that we could not gain by only looking at the data in tabular format. In future cases, you will dig deeper into the numerous graphing capabilities of Python and how to integrate them into your data science workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8:\n",
    "\n",
    "Write a script to find and print the month that has the highest average daily trading volume for each symbol. Also include the average volume value corresponding to that month. For example, symbol D has its highest average daily trading volume of 6.437 million in December 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9:\n",
    "\n",
    "We have so far looked at volatility grouped by stock symbol or by year and month. As our data covers several years, it's also interesting to group the data by calendar month, ignoring the year component (e.g. averaging together all Januarys). This allows us to see if some points of the year, on average, are more susceptible to volatile trading patterns.\n",
    "\n",
    "Group the data by month (ignoring the year), and identify :\n",
    "\n",
    "* The month with, on average, the highest volatility\n",
    "* The month with, on average, the lowest volatility\n",
    "* Any general patterns that you notice over the whole year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10:\n",
    "\n",
    "The final point that we're interested in is looking at the days where:\n",
    "\n",
    "* The return is high\n",
    "* Trading volume is low\n",
    "\n",
    "This indicates days where the price moved substantially but without much changing hands.\n",
    "\n",
    "The thresholds that we are interested in are:\n",
    "* Low volume: any day with a trading volume in the bottom 25th percentile\n",
    "* High return: any day where the return is in the opt 75th percentile\n",
    "\n",
    "Write the code necessary to:\n",
    "* Calculate and add a \"High/Low\" variable for Volume Level (low is below 25th percentile)\n",
    "* Calculate and add a \"High/Low\" variable for Return (high is above 75th percentile)\n",
    "\n",
    "Describe what you see in terms of:\n",
    "* How many rows fall into our \"low volume\" definition?\n",
    "* How many rows fall into our \"high return\" definition?\n",
    "* How many rows fall into the combination of high return with low volume definition?\n",
    "* What are the 20 rows with the highest return but with low volume? What do you notice about these trades?\n",
    "\n",
    "(hint, you can use the [sort_values()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) method in `pandas` to sort a DataFrame by a specific column.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Having completed the analysis of the energy sector stock data, we have identified a number of interesting patterns relating volatility to trading volume. Specifically, we found that periods of high volatility also exhibit very high volume. This trend is consistent across all symbols.\n",
    "\n",
    "We also saw that each stock exhibited \"volatility clustering\" â€“ periods of high volatility tend to be clumped together. Each of the stocks experienced high volatility at relatively similar times which suggests some broader market factor may be affecting the energy sector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this case, we've learned the foundations of the ```pandas``` library in Python. We now know how to:\n",
    "\n",
    "1. Read data from CSV files\n",
    "2. Aggregate and manipulate data using ```pandas```\n",
    "3. Analyze summary statistics and gather information from trends across time\n",
    "4. Use ```matplotlib``` to create plots for visual analysis\n",
    "\n",
    "Going forward, we will be consistently using ```pandas``` as a data analysis framework (along with other tools) to build more complex projects and solve critical business problems. It is critical you become as familiar with ```pandas``` as possible and it is imperative that you continue researching/investigating new components of this library after the completion of this program. What we have taught here are only the essential basics of `pandas`; there is still a vast amount of power to the library that you will discover and utilize later on in your development as a data professional.\n",
    "\n",
    "We highly recommend revisiting this case a few more times and going through it from beginning to end with no aid/answers. You should know the various DataFrame/Series methods we introduced here as well as how to carry out common operations on data, such as finding percentiles, before you consider yourself to have \"mastered\" this material."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
